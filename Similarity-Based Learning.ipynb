{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity-Based Learning\n",
    "\n",
    "---\n",
    "\n",
    "*\"If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.\"* -James Whitcomb Riley\n",
    "\n",
    "**Main Idea**: the best way to make predictions is to look at what has worked well in the past and predict the same thing again. Thus, we need to look at feature spaces and measures of similarity.\n",
    "\n",
    "The most common way to measure similarity is to use distance measures: *the closer two instances are, the more similary they are.* However, to calculate distance between two instances, we need to introduce the concept of **feature space** to represent the inputs to the model. \n",
    "\n",
    "** Inductive Bias:**  things that are similar also have the same target feature values\n",
    "\n",
    "## Feature Space\n",
    "---\n",
    "\n",
    "Suppose we have two descriptive features- Weight and Height- that attempt to predict if a person is male or female. We can represent each feature as an axis on a coordinate system and plot each data point on this x-y plane. The actual gender is indicated on the graph. For example, the feature space would be:\n",
    "\n",
    "<img src=\"https://blogs.sas.com/content/graphicallyspeaking/files/2016/10/ScatterPlotGroupImageMarkers.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Each descriptive feature always takes up a dimension. So if there are 3 features, the feature space would be three-dimensional. Obviously, if there are more features, we would not be able to visualize them, but the idea remains the same.\n",
    "\n",
    "So now we have the official defintion of feature space.\n",
    "    \n",
    "**Feature Space**: m-dimensional space that is created by making each descriptive feature in a dataset an axis of an m-dimensional coordinate system and mapping each instance in the dataset to a point in this coordinate system based on the values of its descriptive features.\n",
    "\n",
    "**Advantages of Feature Spaces**\n",
    "* If the values of the descriptive features of two or more instances in the dataset are the same, then these instances will be mapped to the same points in the feature space.\n",
    "* As the differences between the values of the descriptive features grows, so too does the distance between the points in the feature space that represent these instances.\n",
    "* Thus, the distance between two points in the feature space is a useful mesaure of similarity of the descriptive features of the two instances.\n",
    "\n",
    "## Distance Metrics to Measure Similiarity \n",
    "---\n",
    "\n",
    "\n",
    "The simplest way to measure similarity between two instances, a and b, is to mesaure the distance between the instances in a feature space.\n",
    "\n",
    "**Euclidean Distance**- compute the length of the straight line between two points\n",
    "$$Euclidean(a,b) = \\sqrt{\\sum_{i=1}^{m}(a[i] - b[i])^2 }$$\n",
    "\n",
    "**Manhatten Distance**- compute the sum of the absolute difference between two points\n",
    "$$Manhatten(a,b) = \\sum_{i=1}^{m}abs(a[i] - b[i])$$\n",
    "\n",
    "![Metrics](http://www.improvedoutcomes.com/docs/WebSiteDocs/image/diagram_euclidean_manhattan_distance_metrics.gif)\n",
    "\n",
    "**Minkowski Distance**- both the Euclidean and Manhatten are special cases of the Minkowski Distance. This is a family of distance metrics based on differences between features. By adjusting the paramter p, we can define an infinte number of different distance metrics: Manhatten (p=1) and Euclidean (p=2). Larger values of p place more emphasis on large differences between feature values than smaller values of p.\n",
    "$$Minkowski(a,b) = \\big(\\sum_{i=1}^{m} abs(a[i] - b[i])^p\\big) ^ \\frac{1}{p}$$\n",
    "\n",
    "Despite having infinte distance metrics, Euclidean and Manhatten are the most common. Computationally, Manhatten has a slight advantage over Euclidean, but ***Euclidean is often used as the default***.\n",
    "\n",
    "\n",
    "## Standard Approach: Nearest Neighbour (NN) Algorithm\n",
    "---\n",
    "\n",
    "We can now combine the concepts of feature space and similarity measures to define the Nearest Neighbour Algorithm. \n",
    "\n",
    "**Main Idea**: Essentially, the model looks at the new query, compares it to every instance in the training data, and outputs the target level of the instance that is most similar to the query (the nearest neighbour, or the 1 neighbour).\n",
    "\n",
    "**Training Phase**: storing all the training instances in memory (a list)\n",
    "\n",
    "**Prediction Phase**: the distance in the feature space between the query (new instance) and each instance in memory is computed and the prediction is the target level that is most similar to the query in the feature space.\n",
    "\n",
    "**NN Psuedo Code**\n",
    "\n",
    "**Require**: a set of training instance\n",
    "\n",
    "**Require**: a query instnace\n",
    "\n",
    "1. Iterate across the instances in memory to find the nearest neighbour- this is the instnace with the shortest distance across the feature space to the query instance.\n",
    "2. Make a prediction for the query instance that is equal to the value of the target ffeature of the nearest neighbour.\n",
    "\n",
    "<img src=\"http://www.statsoft.com/portals/0/Support/KNNOverViewImageA.jpg\n",
    "\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "## K-Nearest Neighbours (KNN)\n",
    "\n",
    "---\n",
    "\n",
    "*\"You are the average of the five people you most associate with.\"* -Jim Rohn\n",
    "\n",
    "\n",
    "A way to dampen the noise in the data, we can implement a **k-nearest neighbour algorithm**. \n",
    "\n",
    "Instead of choosing the target level of the closest neighbour, we can use *multiple neighbours* to determine the target level of the new query. We can either use a simple majority vote, or some variation of a weighted majority vote (closer neighbours get a heavier weight, etc...)\n",
    "\n",
    "The nearest neighbour is a special example of KNN, where k=1.\n",
    "\n",
    "## Continuous vs. Categorical Target Features\n",
    "---\n",
    "\n",
    "**Categorical:** take the majority value of the NNs\n",
    "\n",
    "**Continuous:** take the average target value of the NNs\n",
    "\n",
    "\n",
    "Pros and Cons\n",
    "---------------\n",
    "---\n",
    "\n",
    "**Pros**\n",
    "* Simple to understand and implement\n",
    "* Can quickly adapt to changes in input (lazy learning)\n",
    "* Naturally handles multi-class cases\n",
    "* Does not assume a prior probability distribution on the input data\n",
    "* Can do well in practice with enough representative data\n",
    "\n",
    "**Cons**\n",
    "* Computation Time: With a large dataset, searching for the nearest neighbours may be computationally infeasible\n",
    "* Storage of data\n",
    "* Must know we have a meaningful distance/similarity function\n",
    "* Sensitive if data is localized or imbalanced(not representiative or generalized).\n",
    "\n",
    "\n",
    "## Data Normalization\n",
    "---\n",
    "\n",
    "What do we do when the ranges for different features? A larger range might dominate the distance computations. We can normalize the data so that all the data is on the same scale.\n",
    "\n",
    "By normalizing the data, we control the variation acorss the variances of the features and ensure that each feature can contribute equally to the distance metric. Normalizing is an important thing to do in all of machine learning, not just NN.\n",
    "\n",
    "**Feature Scaling: ** brings all the values into the range (0,1)\n",
    "$$ X' = \\frac {X - X_{min}} {X_{max} - X_{min}} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
