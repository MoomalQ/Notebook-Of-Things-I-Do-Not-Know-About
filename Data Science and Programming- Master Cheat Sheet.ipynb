{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 120 Data Science Interview Questions\n",
    "---\n",
    "\n",
    "1. Analyze this dataset and give me a model that can predict this response variable.\n",
    "\n",
    "2. What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?\n",
    "\n",
    "    - Incorrect predictions and classifier.\n",
    "    \n",
    "    **Dataset Shift/Data Fracture**- when training and test distributions are different\n",
    "    \n",
    "    Types of Dataset Shift\n",
    "        * Covariate Shift- shift in the independent variables\n",
    "        * Prior Probability Shift- shift in the target variable\n",
    "        * Concept Shift- shift in the relationship between the independent and target variable\n",
    "    \n",
    "    ![Covariate Shift](http://laoblogger.com/images/covariate-shift-clipart-6.jpg)\n",
    "\n",
    "3. What are some ways I can make my model more robust to outliers?\n",
    "    - Model Changes\n",
    "        - Use a model resistant to outliers. Tree-based models are generally not as affected by outliers, while regression-based models are.\n",
    "        - Use a more robust error metric. Minimize the sum of absolute values of errors instead of the sum of squares reduces the influence of outliers.\n",
    "        - Ensemble Methods\n",
    "\n",
    "    - Data Changes\n",
    "        - Remove the outliers\n",
    "        - Transform the data (log, etc...)\n",
    "        - Winsorize- replacing/modifying a specified number of extreme values with a smaller data value (such as assigning the outlier with a lower weight or changing the value so that it is close to other values in the set)\n",
    "        \n",
    "4. What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?\n",
    "\n",
    "    MAE is more robust to outliers and the MSE is more useful if we are concerned about large errors whose consequences are more severe than smaller errors.  \n",
    "&nbsp;   \n",
    "5. What error metric would you use to evaluate how good a binary classifer is? What if the classes are imbalanced? What if there are more than 2 groups?\n",
    "    - True Postive (TN), True Negative (TN), False Positive (FP, or Type I), False Negative (FN, or Type II)\n",
    "    - Simple Accuracy: (TP + TN) / (TP + TN + FP + FN)  \n",
    "    \n",
    "    - When the dataset is imbalanced, accuracy will be deceiving. Must change performance measure.\n",
    "        - Sensitivity/Recall- measures the ability of a test to detect the condition when the condition is present (TP) / (TP + FN)\n",
    "        - Specificity- measure the ability of a test to not detect the condition when the condition is absent (TN) / (TN + FP)\n",
    "        - Precision- TP / (TP + FP)\n",
    "        - Error Rate- 1 - Precision\n",
    "        - F1- $\\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}$\n",
    "        - ROC\n",
    "        - AUC\n",
    "\n",
    "7. What is regularization and where might it be helpful? What is an example of using regularization in a model?\n",
    "\n",
    "    - Regularization is a technique used to prevent overfitting. We add a penalty term to the loss function to prevent the model from becoming too complex or flexible. It also reduces variance without a substantial increase in bias.\n",
    "    - There are 3 types of regularization\n",
    "        1. L1 (Lasso): Error$_{L1}$ = Error + $\\lambda \\sum |\\beta_{i}|$\n",
    "\n",
    "        2. L2 (Ridge): Error$_{L2}$ = Error + $\\lambda \\sum \\beta_{i}^2$\n",
    "        3. L1/L2 (Elastic Net): \n",
    "\n",
    "        -The key is selecting the tuning parameter: $\\lambda$. One way to select the best $\\lambda$ is cross-validation. Select several $\\lambda$s, compute CV for each option, and choose the $\\lambda$ with the lowest CV score.\n",
    "\n",
    "        - Neither Lasso nor Ridge universally outperform each other. Lasso performs better where a relatively small number of predictors have substantial coefficients (because Lasso regularization can force coefficients to 0). The other advantage of Lasso is that is produces simpler and more interpreatable models (sparse models), since it basically performs feature selection. Ridge performs better when the response is a function of many predictors, with coefficients of roughly the same size. We can use CV to determine which method is better for a given dataset.\n",
    "\n",
    "        - It is also best to apply regularization after standardizing (centering/normalizing) the predictors.\n",
    "        Higher the $\\lambda$, flexibility decreases.\n",
    "\n",
    "## Statistical Learning\n",
    "---\n",
    "1. What is statistical learning?\n",
    "\n",
    "    Statistical learning refers to the tools for understanding data. Specifically, the set of approaches for estimating f (the relationship between the input variables and the output variable). For example, f(X) + $\\epsilon$ = Y. To estimate f, we assume to have n data points (training data) and find $\\hat{f}$ such that Y $\\approx$ $\\hat{f}(X)$\n",
    "\n",
    "2. Parametric vs. Nonparametric Models\n",
    "    \n",
    "    Parametric Models\n",
    "    1. Make an assumption about the functional form, or shape, or f. \n",
    "        - e.g. f is linear in X: f(X) = $\\beta_{0}$ + $\\beta_{1}X_{1}$\n",
    "    2. After the model has been selected, we use a procedure that uses training data to fit or train the model\n",
    "    \n",
    "    Advantage: reduces the problem of estimating f down to one of estimating a set of parameters  \n",
    "    Disdadvantage: model chosen will most likely not match the true (unknown) form of f (too rigid or inflexible)  \n",
    "\n",
    "    \n",
    "    Non-Parametric Models\n",
    "    - Do not make explicit assumptions about the functional form of f. By avoiding assumptions, they have the potential to accurately fit a wider range of possible shapes for f \n",
    "    - Disadvantage: requires a large number of observations (far more than parametric) in order to obtain an accurate estimate for f\n",
    "                       \n",
    "3. Inference vs. Prediction\n",
    "The goal of inference is to determine, or infer, the relationship between the input and the output variables. Prediction is more focused on building a model that can accurately predict new observations. In other words, in inference, we care more about the why. In prediction, we care more about the result. Modeling for inference is different than modeling for prediction.\n",
    "\n",
    "4. Supervised vs. Unsupervised\n",
    "\n",
    "    Supervised- every observation has an associated response to \"supervise\" our training  \n",
    "    Unsupervised- no associated response. The goal is to seek to understand the relationship between variables/observations.\n",
    "    \n",
    "5. What is the bias-variance tradeoff?\n",
    "    -Bias: refers to the error that is introduced when the prediction of the model differs from the correct value. In other words, when the model is too limited or inflexible to learn the true signal/function\n",
    "    \n",
    "    -Variance: the amount that $\\hat{f}$ would change if we estimated it using a different training dataset (sensitivity)\n",
    "    \n",
    "    Flexible/more complex models tend to have higher variance but lower bias (and vice versa).\n",
    "    \n",
    "    The goal is to find models with the perfect balance of variance and bias.\n",
    "    \n",
    "    ![Covariate Shift](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
    "    \n",
    "    \n",
    "6. What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? Whatâ€™s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)\n",
    "\n",
    "    - Logistic Regression: simple, inflexible, linear boundary\n",
    "    - Linear Discriminant Analysis\n",
    "    - K Nearest Neighbours (KNN)\n",
    "    - Support Vector Machines\n",
    "    - Decision Tree/Tree Ensembles\n",
    "    - Deep Learning\n",
    "\n",
    "7. Classification vs. Regression\n",
    "    - Classification: response variable is qualitative or categorical. We often predict the probability that each observation falls in one of these categories.\n",
    "    - Regression: repsonse variable is quantitative\n",
    "    \n",
    "## Statistics\n",
    "---\n",
    "\n",
    "2. What is the Central Limit Theorem and why is it important?\n",
    "    \n",
    "    If we sample from a population using a sufficiently large sample size (around n=30 is sufficient), the mean of the samples will be normally distributed (regardless of the distribution of the original population). It is important because according to the CLT, even though we might not know the shape of the distribution where our data comes from, the CLT says that we can treat the sampling distribution as if it were normal. More info [here.](https://www.thoughtco.com/importance-of-the-central-limit-theorem-3126556)  \n",
    "    \n",
    "    The sum of many independent random variables is approximately normally distributed. In practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior. \n",
    "\n",
    "3. What is sampling? How many sampling methods do you know?\n",
    "    \n",
    "    Sampling is the selection of a subset of observations from a population to estimate characteristics of the whole population.\n",
    "    \n",
    "    * Sampling Methods\n",
    "    \n",
    "        * Simple Random Sampling (SRS)- each observation of the population has an equal chance of being selected\n",
    "        * Stratified Sampling- divide the population into homogenous groups (strata), then a probability sample is drawn from each group\n",
    "        * Cluster Sampling- divide the population into naturally occurring groups (clusters), then a SRS of clusters is selected\n",
    "        * Systematic Sampling\n",
    "        * Multistage Sampling  \n",
    "&nbsp;\n",
    "        \n",
    "3. What is the difference between Type I and Type II error?  \n",
    "    Type I Error- \"False Positive\": detects the condition when the condition is absent  \n",
    "    Type II Error- \"False Negative\": does not detect the condition when the condition is present   \n",
    "&nbsp;\n",
    "    \n",
    "4. What is Linear Regression? What do the terms P-value, coefficient, and $R^{2}$ mean?  \n",
    "  \n",
    "    - Linear Regression is the supervised learning task for modeling and predicting continuous, numeric variables.\n",
    "        - Can be updated easily with new data using stochastic gradient descent (SGD) and straightforward to understand\n",
    "    - The goal is to estimate the coefficients such that the resulting line is as close as possible to the n observation points.  \n",
    "    - $R^{2}$ statistic measures the proportion of variance explained vs. total variance and takes a value between 0 and 1. A value of 1 indicates that a large proportion of variability can be explained by the regression while a value of 0 indicates that the regression did not explain much of the variability in the response. Generally, the higher the $R^{2}$, the better the model. However, a better metric would be adjusted $R^{2}$, because $R^{2}$ increases as long as we include more variables/predictors, even if they are not significant in predicting the response.\n",
    "        - $R^{2}$ is measured as $1 - \\frac {RSS} {TSS}$ \n",
    "    - p-value is the level of marginal significance within a statistical hypothesis test representing the probability of the occurrence of a given event. It can help us determine which predictors are significant in determining the response. A small p-value indicates that a relationship exists between the predictor and the response (typically < 0.5 or 0.1)\n",
    "\n",
    "    - Cons\n",
    "        - Performs poorly when there are non-linear relationships\n",
    "\n",
    "## Random\n",
    "---\n",
    "\n",
    "No Free Lunch Theorem- no one algorithm works best for every problem\n",
    "\n",
    "True Positive (TP): detects the condition when the condition is present  \n",
    "True Negative (TN): does not detect the condition when the condition is absent  \n",
    "False Positive (FP): detects the condition when the condition is absent  \n",
    "False Negative (FN): does not detect the condition when the condition is present  \n",
    "\n",
    "\n",
    "\n",
    "## Programming (CTCI)\n",
    "---\n",
    "### Chapter 1- Arrays and Strings\n",
    "\n",
    "- Hash Tables: data structure that maps keys to values for highly efficient lookup\n",
    "    - The hash table has an underlying array and a hash function that maps the key to an integer (which indicates the index in the array)\n",
    "    - To avoid collisions, we store a linked list at each index of the underlying array\n",
    "    - Worst case runtime- O(N)\n",
    "    - Average- O(1)\n",
    "    - We can assume good implementation keeps collisions to a minimum- O(1)\n",
    "    - Could also implemenent with a balanced binary search tree. Guarantees a O(log N) lookup time and uses less space\n",
    "    \n",
    "    \n",
    "- ArrayList & Resizable Arrays\n",
    "    - In some languages, arrays (lists) are automatically resizable. The list grows as you append items.\n",
    "    - O(1) access, O(N) search, insertion, deletion\n",
    "    \n",
    "- StringBuffer/StringBuilder\n",
    "    - Creates an array of all the strings, copying them back to a string only when necessary instead of creating a new copy of the string every time- which is O($N^{2}$)\n",
    "    \n",
    "TODO- implement HashTable, ArrayList, StringBuilder\n",
    "   \n",
    "   \n",
    "### Chapter 2- Linked Lists\n",
    "Data Structure that represents a sequence of nodes\n",
    "\n",
    "Singly Linked List- each node points to the next node in the linked list and stores\n",
    "![Singly LL](https://www.geeksforgeeks.org/wp-content/uploads/gq/2013/03/Linkedlist.png)\n",
    "Doubly Linked List- gives each node pointers to both the next node and the previous node\n",
    "![Doubly LL](https://www.geeksforgeeks.org/wp-content/uploads/gq/2014/03/DLL1.png)\n",
    "\n",
    "Unlike an array, a linked list does not provide constant time access to a particular \"index\" within the list. If you want to find the Kth element, you have to iterate through K elements.\n",
    "\n",
    "Benefit: add/remove items from the beginning of the list in constant time\n",
    "\n",
    "Creating a Singly Linked List\n",
    "\n",
    "`\n",
    "\n",
    "    class Node:\n",
    "        def __init__ (self, data):\n",
    "            self.data = data\n",
    "            self.next = None\n",
    "       \n",
    "    class LinkedList:\n",
    "        def __init__ (self):\n",
    "            self.head = None\n",
    "            self.tail = None  \n",
    "\n",
    "        def add(self, data):\n",
    "            new_node = Node(data)\n",
    "            if self.head == None:\n",
    "                self.head = new_node\n",
    "            elif self.tail != None:\n",
    "                self.tail.next = new_node\n",
    "            self.tail = new_node  \n",
    "\n",
    "        def remove(self, index):\n",
    "            prev = None\n",
    "            node = self.head\n",
    "            i = 0\n",
    "\n",
    "            while (node != None) and (i < index):\n",
    "                prev = node\n",
    "                node = node.next\n",
    "                i += 1\n",
    "            if prev == None:\n",
    "                self.head = node.next\n",
    "            else:\n",
    "                prev.next = node.next\n",
    "`\n",
    "## References \n",
    "---\n",
    "\n",
    "Shan, Carl, et al. 120 Data Science Interview Questions. \n",
    "\n",
    "McDowell, Gayle Laakmann. Cracking the coding interview: 189 programming interview questions and solutions. CareerCup, 2015.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
